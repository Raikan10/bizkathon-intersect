{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used for the lstm model\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size):\n",
    "\t\"\"\"\n",
    "\t\ttrain_test_split splits the data set according to a specified percentage. Differs\n",
    "\t\tFrom sklearn's similar function in that order is preserved, crucial for time series\n",
    "\t\tanalysis as individual observations are not independant.\n",
    "\t\tInputs:\n",
    "\t\t\tdf -- data frame / series consisting of experiment population\n",
    "\t\t\ttest_size -- percentage of data set to use as testing, as such, values in (0,1)\n",
    "\t\tReturns:\n",
    "\t\t\ttraining and testing data set\n",
    "\t\"\"\"\n",
    "\ttest_length = int(df.shape[0]*test_size)\n",
    "\treturn df[:-test_length], df[-test_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, yhat):\n",
    "\t\"\"\"\n",
    "\t\treturns the root mean squared error of a prediction to measure model accuracy\n",
    "\t\"\"\"\n",
    "\treturn np.sqrt(mean_squared_error(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_dataset(data, timestep=1,n_features=1):\n",
    "\t\"\"\"\n",
    "\t\tLSTM models expect input to be in the following format: [samples, timesteps, features]\n",
    "\t\twhere samples refer to sequences of time steps, timesteps measure the number of singular\n",
    "\t\tobservations in one sequence, and features measures the amount of observations at 1 time step\n",
    "\t\tOur data is not originally formatted for this, so we must recast.\n",
    "\t\tInputs:\n",
    "\t\t\tdata -- data to recast to 3D.\n",
    "\t\t\ttimestep -- number of observations per sequence, defualt is 1\n",
    "\t\t\tn_features -- number of observations per timestep, default is 1\n",
    "\t\tReturns:\n",
    "\t\t\tarrays for training and testing your model w/ specified dimensionality\n",
    "\t\"\"\"\n",
    "\tdata_x, data_y = [],[]\n",
    "\tfor i in range(data.shape[0]-timestep):\n",
    "\t\tdata_x.append(data[i:i+timestep,0])\n",
    "\t\tdata_y.append(data[i+timestep,0])\n",
    "\tdata_x, data_y = np.asarray(data_x), np.asarray(data_y)\n",
    "\tdata_x = data_x.reshape((data_x.shape[0],data_x.shape[1],n_features))\n",
    "\treturn data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(data, param):\n",
    "\t\"\"\"\n",
    "\t\tcreates and fits a single layer LSTM model\n",
    "\t\tInputs:\n",
    "\t\t\tdata -- training and testing data\n",
    "\t\t\tparam -- parameters for the model to be passed in\n",
    "\t\tReturns:\n",
    "\t\t\tfitted recurrent neural net build on a single LSTM layer\n",
    "\t\"\"\"\n",
    "\tinputs, nodes, epochs, batch_size, features = param\n",
    "\ttrain, test = data\n",
    "\n",
    "\ttrain_x, train_y = lstm_dataset(train, timestep=inputs,n_features=features)\n",
    "\ttest_x, test_y = lstm_dataset(test, timestep=inputs,n_features=features)\n",
    "\n",
    "\trnn = Sequential()\n",
    "\trnn.add(LSTM(nodes,input_shape=(inputs, features)))\n",
    "\trnn.add(Dense(1))\n",
    "\trnn.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\thistory = rnn.fit(train_x, train_y, epochs = epochs, batch_size=batch_size,\n",
    "\t\t\t\t\t\t\t\tverbose=0, shuffle=False, validation_data=(test_x,test_y))\n",
    "\treturn rnn, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_validation(data, param):\n",
    "\t\"\"\"\n",
    "\t\tUnlike other ML models, time series forecasting cannot use standard CV methods as they\n",
    "\t\tassume that the observations are independant, which is not the case for time series\n",
    "\t\tanalysis. walk_through_validation fits and forecasts and returns the root mean squared\n",
    "\t\terror of the predictions\n",
    "\t\tInput:\n",
    "\t\t\tdata -- data to be unpacked\n",
    "\t\t\tparam -- parameteres to be passed onto other functions\n",
    "\t\tReturns:\n",
    "\t\t\terror -- root mean squared error of prediction\n",
    "\t\"\"\"\n",
    "\t# unpack data\n",
    "\ttrain, test, scale = data\n",
    "\tinputs,_,_,_, features = param\n",
    "\t# scale data to range (0,1)\n",
    "\ttrain_scaled, test_scaled = scale.fit_transform(train.values.reshape(-1,1)), scale.fit_transform(test.values.reshape(-1,1))\n",
    "\tdata_scaled = train_scaled, test_scaled\n",
    "\t# fit our model\n",
    "\tmodel, history = fit_model(data_scaled, param)\n",
    "\ttest_x, test_y = lstm_dataset(test_scaled, timestep=inputs,n_features=features)\n",
    "\tprediction = model.predict(test_x)\n",
    "\ttest_y = scale.inverse_transform(test_y.reshape(-1,1))\n",
    "\t# rescale predictions back to standard metric\n",
    "\tprediction = scale.inverse_transform(prediction)\n",
    "\terror = rmse(test_y, prediction)\n",
    "\treturn error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evals(data, param, key_dict,evals=1):\n",
    "\t\"\"\"\"\n",
    "\t\tWe run tests with each set of parameters a fixed amount of time and take the average\n",
    "\t\tscore to ensure better results as the weights in our model are randomly initialized.\n",
    "\t\tInputs:\n",
    "\t\t\tdata -- data to be unpacked for additional functins\n",
    "\t\t\tparam -- parameters to be evaluated\n",
    "\t\t\tkey_dict -- dictionary with hyperparameter values\n",
    "\t\t\tevals -- number of iterations to run over a single parameter\n",
    "\t\tReturns:\n",
    "\t\t\tkey -- string of the parameters used for specific evaluation\n",
    "\t\t\tscore -- average score across all iterations\n",
    "\t\"\"\"\n",
    "\tkey = str(param)\n",
    "\n",
    "\tscores = [walk_through_validation(data, param) for _ in range(evals)]\n",
    "\tkey_dict[key] = param\n",
    "\tprint(\"Hyperparams {} evaluated!\".format(param))\n",
    "\treturn key, param, np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(data, params):\n",
    "\t\"\"\"\n",
    "\t\tgrid search is used to find the optimal batch of parameters for our model. We make our\n",
    "\t\town funciton instead of a sklearn one for the same reason we recreated train_test_split\n",
    "\t\tInput:\n",
    "\t\t\tdata -- data to be unpacked\n",
    "\t\t\tparams -- list of all combinations of parameters to be tested\n",
    "\t\tReturns:\n",
    "\t\t\tresults of all parameters alongside their corresponding error value\n",
    "\t\"\"\"\n",
    "\tkey_dict = {}\n",
    "\tresults = [model_evals(data, param, key_dict) for param in params] # iterates through all parameter combos\n",
    "\tresults.sort(key=lambda x: x[2]) # sorts list based off of rmse\n",
    "\n",
    "\treturn results\n",
    "\n",
    "def parameters(inputs, nodes, epochs, batch_size, features):\n",
    "\t\"\"\"\n",
    "\t\tCreates list of all possible combinatinos of parameters\n",
    "\t\tInputs:\n",
    "\t\t\tparameters -- all stated parameters for the analysis\n",
    "\t\tReturns:\n",
    "\t\t\tparams -- list of all combinations of parameters\n",
    "\t\"\"\"\n",
    "\tparams = []\n",
    "\tfor a in inputs:\n",
    "\t\tfor b in nodes:\n",
    "\t\t\tfor c in epochs:\n",
    "\t\t\t\tfor d in batch_size:\n",
    "\t\t\t\t\tfor e in features:\n",
    "\t\t\t\t\t\tparams.append([a,b,c,d,e])\n",
    "\treturn params\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learned dataset.\n",
    "\tInputs:\n",
    "\t\tdata -- sequence of observations as a list or numpy array.\n",
    "\t\tn_in -- number of lag observations as input (x)\n",
    "\t\tn_out -- number of observations as output (y)\n",
    "\t\tdropnan -- boolean whether or not to drop rows with nan values\n",
    "\t Returns:\n",
    "\t \tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = [], []\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var{}(t-{})'.format(j+1, i)) for j in range(n_vars)]\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var{}(t)'.format(j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var{}(t+{})'.format(j+1, i)) for j in range(n_vars)]\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
